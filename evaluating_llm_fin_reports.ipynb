{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eeea0fc",
   "metadata": {},
   "source": [
    "Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd8f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from statistics import mean\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Literal\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608e344",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ba2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (parameterized)\n",
    "PROVIDER = \"gemini\"  # Options: \"openai\", \"gemini\", \"anthropic\"\n",
    "TASK_MODEL_TEMP = 0.0  # Low temp for deterministic extraction\n",
    "REFLECTION_MODEL_TEMP = 1.0  # High temp for diverse reflections\n",
    "\n",
    "# Paths\n",
    "QUESTIONS = \"quetions\"\n",
    "MARKDOWN_DIR = Path(\"fin_docs\")\n",
    "PDF_DIR = Path(\"financial_pdfs/raw\")\n",
    "\n",
    "# Data split\n",
    "TRAIN_RATIO = 0.75\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# GEPA configuration\n",
    "GEPA_BUDGET = \"light\"  # Start with light, can increase to \"medium\" or \"heavy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43428fe3",
   "metadata": {},
   "source": [
    "Initialize DSPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2747f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task Model: gemini-2.5-flash\n",
      "✅ Reflection Model: gemini-2.5.pro\n"
     ]
    }
   ],
   "source": [
    "# Get API key from environment\n",
    "if PROVIDER == \"openai\":\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    TASK_MODEL = \"gpt-4.1-mini\"  # Model for the extraction task\n",
    "    REFLECTION_MODEL = \"gpt-4.1\"  # Model for GEPA reflection\n",
    "elif PROVIDER == \"gemini\":\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    TASK_MODEL = \"gemini-2.5-flash\"  # Model for the extraction task\n",
    "    REFLECTION_MODEL = \"gemini-2.5.pro\"  # Model for GEPA reflection\n",
    "elif PROVIDER == \"anthropic\":\n",
    "    api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    TASK_MODEL = \"claude-3\"  # Model for the extraction task\n",
    "    REFLECTION_MODEL = \"claude-3\"  # Model for GEPA reflection\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported provider: {PROVIDER}\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found in environment variables\")\n",
    "\n",
    "# Configure task model\n",
    "task_lm = dspy.LM(\n",
    "    model=f\"{PROVIDER}/{TASK_MODEL}\",\n",
    "    temperature=TASK_MODEL_TEMP,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Configure reflection model for GEPA\n",
    "reflection_lm = dspy.LM(\n",
    "    model=f\"{PROVIDER}/{REFLECTION_MODEL}\",\n",
    "    temperature=REFLECTION_MODEL_TEMP,\n",
    "    max_tokens=32000,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Set default model\n",
    "dspy.configure(lm=task_lm)\n",
    "\n",
    "print(f\"✅ Task Model: {TASK_MODEL}\")\n",
    "print(f\"✅ Reflection Model: {REFLECTION_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02256df",
   "metadata": {},
   "source": [
    "Load Markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75fa8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 60 Markdown files.\n"
     ]
    }
   ],
   "source": [
    "# List Markdown files\n",
    "markdown_files = list(MARKDOWN_DIR.glob(\"*.md\"))\n",
    "\n",
    "# Load Markdown file\n",
    "def load_markdown_file(file_path: Path) -> str:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Load all Markdown files\n",
    "markdown_contents = {md_file.name: load_markdown_file(md_file) for md_file in markdown_files}\n",
    "\n",
    "# Count loaded files\n",
    "print(f\"✅ Loaded {len(markdown_contents)} Markdown files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e4bbf",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aae5d7",
   "metadata": {},
   "source": [
    "## Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1822698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvestorQuestion(BaseModel):\n",
    "    \"\"\"A single question an investor would ask\"\"\"\n",
    "    question: str = Field(description=\"The question text\")\n",
    "    category: str = Field(description=\"Category: financial, competitive, management, or risk\")\n",
    "    importance: float = Field(description=\"Importance weight 0-1\", ge=0, le=1)\n",
    "    reasoning: str = Field(description=\"Why this question matters for the investment decision\")\n",
    "\n",
    "\n",
    "class QuestionList(BaseModel):\n",
    "    \"\"\"Collection of investor questions\"\"\"\n",
    "    questions: List[InvestorQuestion]\n",
    "\n",
    "\n",
    "class QuestionEvaluation(BaseModel):\n",
    "    \"\"\"Evaluation of how well a report answers a question\"\"\"\n",
    "    answerable: Literal[\"yes\", \"partial\", \"no\"] = Field(\n",
    "        description=\"Can the question be answered from the report?\"\n",
    "    )\n",
    "    answer: str = Field(description=\"The answer extracted from the report, or explanation of what's missing\")\n",
    "    evidence: List[str] = Field(description=\"Direct quotes/references from the report supporting the answer\")\n",
    "    missing_information: List[str] = Field(description=\"Key information needed but not found in report\")\n",
    "    quality_rating: int = Field(description=\"Quality score 0-10\", ge=0, le=10)\n",
    "    reasoning: str = Field(description=\"Explanation of the rating\")\n",
    "\n",
    "\n",
    "class ReportMetrics(BaseModel):\n",
    "    \"\"\"Aggregate metrics for a report\"\"\"\n",
    "    coverage_rate: float = Field(description=\"Percentage of questions answered (0-1)\")\n",
    "    quality_score: float = Field(description=\"Average quality rating (0-10)\")\n",
    "    answerable_fully: int = Field(description=\"Count of fully answerable questions\")\n",
    "    answerable_partial: int = Field(description=\"Count of partially answerable questions\")\n",
    "    not_answerable: int = Field(description=\"Count of unanswerable questions\")\n",
    "    critical_gaps: List[str] = Field(description=\"Most important missing information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283bb676",
   "metadata": {},
   "source": [
    "## DSPY Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ea5737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateExpertQuestions(dspy.Signature):\n",
    "    \"\"\"Generate critical questions that a specific investment persona would ask about a company\"\"\"\n",
    "    \n",
    "    persona_name: str = dspy.InputField(description=\"Name of the investment persona (e.g., Warren Buffett)\")\n",
    "    persona_philosophy: str = dspy.InputField(description=\"Investment philosophy and approach\")\n",
    "    company_summary: str = dspy.InputField(description=\"Basic company information and context\")\n",
    "    n_questions: int = dspy.InputField(description=\"Number of questions to generate\")\n",
    "    \n",
    "    questions: QuestionList = dspy.OutputField(description=\"List of critical investment questions\")\n",
    "\n",
    "\n",
    "class EvaluateReportCoverage(dspy.Signature):\n",
    "    \"\"\"Evaluate whether and how well a report answers a specific investment question\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(description=\"The investment question to evaluate\")\n",
    "    question_category: str = dspy.InputField(description=\"Question category for context\")\n",
    "    report: str = dspy.InputField(description=\"The investment report to evaluate\")\n",
    "    persona_name: str = dspy.InputField(description=\"Investor persona for context\")\n",
    "    \n",
    "    evaluation: QuestionEvaluation = dspy.OutputField(\n",
    "        description=\"Detailed evaluation of question coverage\"\n",
    "    )\n",
    "\n",
    "\n",
    "class AssessPersonaAlignment(dspy.Signature):\n",
    "    \"\"\"Assess whether a report reflects the investment philosophy of the persona\"\"\"\n",
    "    \n",
    "    persona_name: str = dspy.InputField(description=\"Investment persona\")\n",
    "    persona_philosophy: str = dspy.InputField(description=\"Core investment principles\")\n",
    "    report: str = dspy.InputField(description=\"Investment report\")\n",
    "    questions_coverage: str = dspy.InputField(description=\"Summary of question coverage\")\n",
    "    \n",
    "    alignment_score: float = dspy.OutputField(description=\"Alignment score 0-10\")\n",
    "    alignment_reasoning: str = dspy.OutputField(description=\"Explanation of alignment assessment\")\n",
    "    philosophy_evidence: List[str] = dspy.OutputField(description=\"Report sections reflecting philosophy\")\n",
    "    philosophy_gaps: List[str] = dspy.OutputField(description=\"Missing philosophy elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165f107",
   "metadata": {},
   "source": [
    "## DSPY Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b5f8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator(dspy.Module):\n",
    "    \"\"\"Module for generating expert investor questions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(GenerateExpertQuestions)\n",
    "    \n",
    "    def forward(self, persona_name: str, persona_philosophy: str, \n",
    "                company_summary: str, n_questions: int = 15) -> QuestionList:\n",
    "        result = self.generate(\n",
    "            persona_name=persona_name,\n",
    "            persona_philosophy=persona_philosophy,\n",
    "            company_summary=company_summary,\n",
    "            n_questions=n_questions\n",
    "        )\n",
    "        return result.questions\n",
    "\n",
    "\n",
    "class CoverageEvaluator(dspy.Module):\n",
    "    \"\"\"Module for evaluating report coverage of a single question\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.evaluate = dspy.ChainOfThought(EvaluateReportCoverage)\n",
    "    \n",
    "    def forward(self, question: InvestorQuestion, report: str, \n",
    "                persona_name: str) -> QuestionEvaluation:\n",
    "        result = self.evaluate(\n",
    "            question=question.question,\n",
    "            question_category=question.category,\n",
    "            report=report,\n",
    "            persona_name=persona_name\n",
    "        )\n",
    "        return result.evaluation\n",
    "\n",
    "\n",
    "class PersonaAlignmentChecker(dspy.Module):\n",
    "    \"\"\"Module for checking persona alignment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.assess = dspy.ChainOfThought(AssessPersonaAlignment)\n",
    "    \n",
    "    def forward(self, persona_name: str, persona_philosophy: str,\n",
    "                report: str, questions_coverage: str):\n",
    "        return self.assess(\n",
    "            persona_name=persona_name,\n",
    "            persona_philosophy=persona_philosophy,\n",
    "            report=report,\n",
    "            questions_coverage=questions_coverage\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7fbc6c",
   "metadata": {},
   "source": [
    "## Main Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20cb7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportEvaluator(dspy.Module):\n",
    "    \"\"\"Complete report evaluation pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, persona_definitions: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            persona_definitions: Dict mapping persona names to their philosophies\n",
    "                e.g., {\"Warren Buffett\": \"Focus on quality businesses with moats...\"}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.persona_definitions = persona_definitions\n",
    "        \n",
    "        # Initialize sub-modules\n",
    "        self.question_generator = QuestionGenerator()\n",
    "        self.coverage_evaluator = CoverageEvaluator()\n",
    "        self.alignment_checker = PersonaAlignmentChecker()\n",
    "        \n",
    "        # Storage\n",
    "        self.questions = []\n",
    "        self.evaluation_results = []\n",
    "    \n",
    "    def generate_expert_questions(self, persona_name: str, \n",
    "                                 company_summary: str, n: int = 15) -> List[InvestorQuestion]:\n",
    "        \"\"\"Generate expert questions for the persona\"\"\"\n",
    "        persona_philosophy = self.persona_definitions[persona_name]\n",
    "        \n",
    "        question_list = self.question_generator(\n",
    "            persona_name=persona_name,\n",
    "            persona_philosophy=persona_philosophy,\n",
    "            company_summary=company_summary,\n",
    "            n_questions=n\n",
    "        )\n",
    "        \n",
    "        self.questions = question_list.questions\n",
    "        return self.questions\n",
    "    \n",
    "    def evaluate_report_coverage(self, report: str, persona_name: str) -> List[QuestionEvaluation]:\n",
    "        \"\"\"Evaluate how well the report covers each question\"\"\"\n",
    "        self.evaluation_results = []\n",
    "        \n",
    "        for question in self.questions:\n",
    "            evaluation = self.coverage_evaluator(\n",
    "                question=question,\n",
    "                report=report,\n",
    "                persona_name=persona_name\n",
    "            )\n",
    "            self.evaluation_results.append(evaluation)\n",
    "        \n",
    "        return self.evaluation_results\n",
    "    \n",
    "    def compute_metrics(self, persona_name: str, report: str) -> ReportMetrics:\n",
    "        \"\"\"Compute aggregate metrics\"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            raise ValueError(\"Must run evaluate_report_coverage first\")\n",
    "        \n",
    "        # Count answerability\n",
    "        answerable_counts = {\n",
    "            \"yes\": sum(1 for r in self.evaluation_results if r.answerable == \"yes\"),\n",
    "            \"partial\": sum(1 for r in self.evaluation_results if r.answerable == \"partial\"),\n",
    "            \"no\": sum(1 for r in self.evaluation_results if r.answerable == \"no\")\n",
    "        }\n",
    "        \n",
    "        total = len(self.evaluation_results)\n",
    "        coverage_rate = (answerable_counts[\"yes\"] + 0.5 * answerable_counts[\"partial\"]) / total\n",
    "        \n",
    "        # Average quality\n",
    "        quality_score = mean([r.quality_rating for r in self.evaluation_results])\n",
    "        \n",
    "        # Identify critical gaps (questions with high importance and low coverage)\n",
    "        critical_gaps = []\n",
    "        for q, eval_result in zip(self.questions, self.evaluation_results):\n",
    "            if q.importance > 0.7 and eval_result.quality_rating < 5:\n",
    "                critical_gaps.extend(eval_result.missing_information)\n",
    "        \n",
    "        # Get persona alignment\n",
    "        persona_philosophy = self.persona_definitions[persona_name]\n",
    "        coverage_summary = self._summarize_coverage()\n",
    "        \n",
    "        alignment = self.alignment_checker(\n",
    "            persona_name=persona_name,\n",
    "            persona_philosophy=persona_philosophy,\n",
    "            report=report,\n",
    "            questions_coverage=coverage_summary\n",
    "        )\n",
    "        \n",
    "        return ReportMetrics(\n",
    "            coverage_rate=coverage_rate,\n",
    "            quality_score=quality_score,\n",
    "            answerable_fully=answerable_counts[\"yes\"],\n",
    "            answerable_partial=answerable_counts[\"partial\"],\n",
    "            not_answerable=answerable_counts[\"no\"],\n",
    "            critical_gaps=list(set(critical_gaps))[:5]  # Top 5 unique gaps\n",
    "        ), alignment\n",
    "    \n",
    "    def _summarize_coverage(self) -> str:\n",
    "        \"\"\"Create a summary of question coverage for persona alignment\"\"\"\n",
    "        summary_parts = []\n",
    "        for q, eval_result in zip(self.questions, self.evaluation_results):\n",
    "            summary_parts.append(\n",
    "                f\"Q: {q.question}\\n\"\n",
    "                f\"Coverage: {eval_result.answerable}\\n\"\n",
    "                f\"Quality: {eval_result.quality_rating}/10\\n\"\n",
    "            )\n",
    "        return \"\\n\".join(summary_parts)\n",
    "    \n",
    "    def forward(self, persona_name: str, company_summary: str, \n",
    "                report: str, n_questions: int = 15):\n",
    "        \"\"\"Complete evaluation pipeline\"\"\"\n",
    "        # Generate questions\n",
    "        self.generate_expert_questions(persona_name, company_summary, n_questions)\n",
    "        \n",
    "        # Evaluate coverage\n",
    "        self.evaluate_report_coverage(report, persona_name)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics, alignment = self.compute_metrics(persona_name, report)\n",
    "        \n",
    "        return {\n",
    "            \"questions\": self.questions,\n",
    "            \"evaluations\": self.evaluation_results,\n",
    "            \"metrics\": metrics,\n",
    "            \"alignment\": alignment\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9474ec3",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_usage():\n",
    "    \"\"\"Example of how to use the ReportEvaluator\"\"\"\n",
    "    \n",
    "    # Configure DSPy\n",
    "    lm = dspy.LM(f'{PROVIDER/{TASK_MODEL}}', max_tokens=4000)\n",
    "    dspy.configure(lm=lm)\n",
    "    \n",
    "    # Define personas\n",
    "    persona_definitions = {\n",
    "        \"Warren Buffett\": \"\"\"\n",
    "        Focus on high-quality businesses with durable competitive advantages.\n",
    "        Key criteria: strong economic moat, high ROE (>15%), consistent earnings,\n",
    "        low debt, excellent management. Buy and hold forever mentality.\n",
    "        \"\"\",\n",
    "        \"Benjamin Graham\": \"\"\"\n",
    "        Deep value with margin of safety. Focus on price < intrinsic value.\n",
    "        Key criteria: P/B < 1.0, P/E < 10, strong balance sheet, low debt,\n",
    "        tangible asset backing. Seek distressed opportunities.\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ReportEvaluator(persona_definitions)\n",
    "    \n",
    "    # Example data\n",
    "    company_summary = \"\"\"\n",
    "    GraceKennedy Ltd: Jamaican conglomerate with operations in food trading,\n",
    "    banking, insurance, and remittances. Market cap: $50B JMD.\n",
    "    Established 1922, listed on JSE Main Market.\n",
    "    \"\"\"\n",
    "    \n",
    "    report = \"\"\"\n",
    "    [Your generated investment report here]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator(\n",
    "        persona_name=\"Warren Buffett\",\n",
    "        company_summary=company_summary,\n",
    "        report=report,\n",
    "        n_questions=15\n",
    "    )\n",
    "    \n",
    "    # Access results\n",
    "    print(f\"Coverage Rate: {results['metrics'].coverage_rate:.2%}\")\n",
    "    print(f\"Quality Score: {results['metrics'].quality_score:.1f}/10\")\n",
    "    print(f\"Persona Alignment: {results['alignment'].alignment_score:.1f}/10\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5753e98",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb95889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZATION WITH DSPY\n",
    "# ============================================================================\n",
    "\n",
    "class ReportQualityMetric:\n",
    "    \"\"\"Metric for optimizing report evaluation\"\"\"\n",
    "    \n",
    "    def __call__(self, example, prediction, trace=None) -> float:\n",
    "        \"\"\"\n",
    "        Score based on:\n",
    "        - Coverage rate (40%)\n",
    "        - Quality score (40%)\n",
    "        - Persona alignment (20%)\n",
    "        \"\"\"\n",
    "        metrics = prediction['metrics']\n",
    "        alignment = prediction['alignment']\n",
    "        \n",
    "        coverage_score = metrics.coverage_rate\n",
    "        quality_score = metrics.quality_score / 10  # Normalize to 0-1\n",
    "        alignment_score = alignment.alignment_score / 10\n",
    "        \n",
    "        total_score = (\n",
    "            0.4 * coverage_score +\n",
    "            0.4 * quality_score +\n",
    "            0.2 * alignment_score\n",
    "        )\n",
    "        \n",
    "        return total_score\n",
    "\n",
    "\n",
    "def optimize_evaluator(train_examples, evaluator):\n",
    "    \"\"\"\n",
    "    Optimize the evaluator using DSPy's optimizer\n",
    "    \n",
    "    Args:\n",
    "        train_examples: List of dspy.Example objects with:\n",
    "            - persona_name\n",
    "            - company_summary\n",
    "            - report\n",
    "            - (optionally) expected_metrics for supervision\n",
    "    \"\"\"\n",
    "    from dspy.teleprompt import BootstrapFewShot\n",
    "    \n",
    "    metric = ReportQualityMetric()\n",
    "    \n",
    "    # Optimize with few-shot examples\n",
    "    optimizer = BootstrapFewShot(\n",
    "        metric=metric,\n",
    "        max_bootstrapped_demos=4,\n",
    "        max_labeled_demos=4\n",
    "    )\n",
    "    \n",
    "    optimized_evaluator = optimizer.compile(\n",
    "        evaluator,\n",
    "        trainset=train_examples\n",
    "    )\n",
    "    \n",
    "    return optimized_evaluator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
